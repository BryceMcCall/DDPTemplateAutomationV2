{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "import ast\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering function for report_df\n",
    "def correct_list_string(s):\n",
    "    # Check if the string starts with [ and ends with ] to ensure it looks like a list\n",
    "    s = s.replace('?', '')\n",
    "\n",
    "    if s.startswith('[') and s.endswith(']'):\n",
    "        # Extract the contents inside the brackets and split them by comma\n",
    "        items = s[1:-1].split(',')\n",
    "        \n",
    "        # Process each item to ensure it has single quotes around it\n",
    "        corrected_items = []\n",
    "        for item in items:\n",
    "            # Strip spaces and any extra single quotes\n",
    "            item_stripped = item.strip().strip(\"'\")\n",
    "            corrected_items.append(f\"'{item_stripped}'\")\n",
    "        \n",
    "        # Reconstruct the list string\n",
    "        return '[' + ', '.join(corrected_items) + ']'\n",
    "    return s\n",
    "\n",
    "def filter_data(row):\n",
    "    conditions = []\n",
    "    debug_info = [] # To collect debug information\n",
    "\n",
    "    # Create the unique ID from the filter_df's first three columns\n",
    "    unique_id = f\"{row['WorkbookID']}_{row['SheetID']}_{row['RowID']}\"\n",
    "\n",
    "    for col, value in row.items():\n",
    "        # Skip the first three columns, columns that are not in report_df, columns with '*' as value, \n",
    "        # and the 'Year' column\n",
    "        if col in ['WorkbookID', 'SheetID', 'RowID'] or col not in report_df.columns or value == \"*\":\n",
    "            continue\n",
    "\n",
    "         # If value is a string, remove spaces at the end\n",
    "        if isinstance(value, str):\n",
    "            value = value.rstrip()\n",
    "\n",
    "        # Check and correct the string representation of list if necessary\n",
    "        if isinstance(value, str) and value.startswith(\"[\"):\n",
    "            if col != 'Year':  # Only correct if the column is not 'Year'\n",
    "                value = correct_list_string(value)  # Correct the string representation\n",
    "            value = ast.literal_eval(value)  # Convert the string to an actual list\n",
    "\n",
    "        # If value is a list, check if the column value in report_df is in the list\n",
    "        if isinstance(value, list):\n",
    "            conditions.append(report_df[col].isin(value))\n",
    "            debug_info.append(f\"{col} is in {value}\") # For debugging\n",
    "        else:\n",
    "            conditions.append(report_df[col] == value)\n",
    "            debug_info.append(f\"{col} == {value}\") # For debugging\n",
    "            \n",
    "\n",
    "    \n",
    "    # If there are no conditions to check, return the entire report_df\n",
    "    if not conditions:\n",
    "        return report_df\n",
    "    \n",
    "    # Combine all conditions with an AND operator\n",
    "    combined_condition = conditions[0]\n",
    "    for cond in conditions[1:]:\n",
    "        combined_condition &= cond\n",
    "\n",
    "    result = report_df[combined_condition].copy()  # Ensure not to modify the original dataframe\n",
    "    result['UniqueID'] = unique_id  # Add the unique ID to the filtered result\n",
    "    if result.empty:\n",
    "        print(f\"Filter {debug_info} resulted in empty dataframe.\") # This will show the failed conditions\n",
    "        \n",
    "\n",
    "    return result\n",
    "\n",
    "# Convert SATIMGE values based on Unit column\n",
    "def convert_value(row):\n",
    "    factor = conversion_factors.get(row['TargetUnit'])\n",
    "    if factor:\n",
    "        result_value = row['SATIMGE'] * factor\n",
    "        return np.round(result_value, 6)\n",
    "    # If unit not found in conversion_factors, return original value\n",
    "    return row['SATIMGE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_Report = \"input/REPORT_00.csv\" # add/replace file path here or filename if in same directory\n",
    "path_SetsAndMaps = \"input/SetsAndMaps.xlsm\" # add/replace file path here or filename if in same directory\n",
    "path_Output = \"input/merged_data.csv\" # add/replace file path here or filename if in same directory\n",
    "\n",
    "# Read in the DataFrames\n",
    "report_df = pd.read_csv(path_Report, low_memory=False)\n",
    "mapPRC_df = pd.read_excel(path_SetsAndMaps, sheet_name=\"mapPRC\")\n",
    "mapCOM_df = pd.read_excel(path_SetsAndMaps, sheet_name=\"mapCOM\")\n",
    "\n",
    "# Replace 'Eps' values with 0\n",
    "report_df['SATIMGE'] = report_df['SATIMGE'].replace('Eps', 0)\n",
    "report_df['SATIMGE'] = report_df['SATIMGE'].astype(float)\n",
    "\n",
    "# Merge the DataFrames\n",
    "merged_df = report_df.merge(mapPRC_df, on='Process', how='left')\n",
    "merged_df = merged_df.merge(mapCOM_df, on='Commodity', how='left')\n",
    "\n",
    "# Reset index\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# merged_df.to_csv(path_Output, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADJUST THE EMISSIONS TO CO2EQ:\n",
    "\n",
    "# set the emissions factors\n",
    "emf_df = pd.DataFrame({'Indicator': ['CH4','N2O','C2F6','CF4'],\n",
    "                    'E_factor': [28, 265,11100,6630]})\n",
    "\n",
    "# Merge the two DataFrames based on the 'Indicator' column\n",
    "merged_df = merged_df.merge(emf_df, on='Indicator', how='left')\n",
    "\n",
    "# Fill missing values in 'Multiplier' column with 1\n",
    "merged_df['E_factor'].fillna(1, inplace=True)\n",
    "\n",
    "# Multiply the values from 'SATIMGE' by the 'E_factor' column\n",
    "merged_df['Result'] = merged_df['SATIMGE'] * merged_df['E_factor']\n",
    "\n",
    "# Optionally, drop the 'Multiplier' column or rename columns\n",
    "merged_df = merged_df.drop('SATIMGE', axis=1) #dropping the original SATIMGE values.\n",
    "merged_df = merged_df.drop('E_factor', axis=1)\n",
    "#rename to new SATIMGE\n",
    "merged_df = merged_df.rename(columns={'Result': 'SATIMGE'}) #new SATIMGE values\n",
    "\n",
    "# Reset index\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "merged_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = merged_df['Scenario'].unique() #array \n",
    "scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#path where all filters are\n",
    "filter_folder = 'input\\\\FilterScripts\\\\'\n",
    "curr_wdr = os.getcwd()\n",
    "filters_path = os.path.join(curr_wdr,filter_folder)\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Iterate over files in the directory\n",
    "for filename in os.listdir(filters_path):\n",
    "    if filename.endswith('.xlsx'):\n",
    "        print(filename)\n",
    "        file_path = os.path.join(filters_path, filename)\n",
    "        df = pd.read_excel(file_path)\n",
    "        df = df.dropna()\n",
    "        dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "combined_df = combined_df.dropna()\n",
    "\n",
    "# Fill NaN values with 0 for the specified columns\n",
    "columns_to_convert = ['WorkbookID', 'SheetID', 'RowID']\n",
    "\n",
    "if not combined_df.empty:\n",
    "    combined_df[columns_to_convert] = combined_df[columns_to_convert].fillna(0).astype(int)\n",
    "\n",
    "# Split the 'TargetUnit' column if needed\n",
    "Units = combined_df[['WorkbookID', 'SheetID', 'RowID', 'TargetUnit']]\n",
    "combined_df = combined_df.drop(columns=['TargetUnit'])\n",
    "\n",
    "# Reset the index\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Now, 'combined_df' contains data from all the Excel files in the folder\n",
    "filter_df = combined_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rename the scenarios according to Workbook ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of WorkbookID values to corresponding scenario names\n",
    "workbook_scenario_mapping = {\n",
    "    1: 'NZ10_2099B_99NESSNXCT-IMG',\n",
    "    2: 'NZ10_2050A_08EESSNXCT-IMG',\n",
    "    3: 'NZ10_2050A_09EESSNXCT-IMG'\n",
    "}\n",
    "\n",
    "# Define a function to update the 'Scenario' column based on conditions\n",
    "def update_scenario(row):\n",
    "    scenario = row['Scenario']\n",
    "    workbook_id = row['WorkbookID']\n",
    "    \n",
    "    # Check if the scenario name is in the mapping and matches the WorkbookID\n",
    "    if scenario in workbook_scenario_mapping and workbook_scenario_mapping[workbook_id] == scenario:\n",
    "        return scenario  # No change is needed\n",
    "        \n",
    "    # Check if the scenario name is in the mapping but doesn't match the WorkbookID\n",
    "    if scenario in workbook_scenario_mapping and workbook_scenario_mapping[workbook_id] != scenario:\n",
    "        return workbook_scenario_mapping[workbook_id]  # Update to the correct scenario\n",
    "    \n",
    "    # If the scenario is not in the mapping, return the scenario based on WorkbookID\n",
    "    if workbook_id in workbook_scenario_mapping:\n",
    "        return workbook_scenario_mapping[workbook_id]\n",
    "    \n",
    "    # If neither scenario nor WorkbookID are in the mapping, return the existing scenario\n",
    "    return scenario\n",
    "\n",
    "# Update the 'Scenario' column in filter_df based on the defined conditions\n",
    "filter_df['Scenario'] = filter_df.apply(update_scenario, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#testing an edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Units['UniqueID'] = Units['WorkbookID'].astype(str) + '_' + Units['SheetID'].astype(str) + '_' + Units['RowID'].astype(str)\n",
    "Units.drop(columns=['WorkbookID', 'SheetID', 'RowID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Units['UniqueID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Process', 'Commodity', 'Year', 'Scenario', 'Indicator',\n",
    "       'Sector', 'Subsector', 'Subsubsector', 'TechDescription',\n",
    "       'IPCC_Category_L1', 'IPCC_Category_L2', 'IPCC_Category_L3',\n",
    "       'IPCC_Category_L4', 'Description', 'Short Description',\n",
    "       'Commodity_Name']\n",
    "\n",
    "for col in columns:\n",
    "    print(col, \" unique values: \\n\", merged_df[col].unique(), \"\\n \\n\", '--------------------------------########################--------------------', '\\n \\n')\n",
    "\n",
    "#ORG_NZ10_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df = merged_df\n",
    "\n",
    "# Instantiate df list\n",
    "filtered_dfs = []\n",
    "i = 1\n",
    "# Iterate through each row in filter_df and filter report_df\n",
    "for _, row in filter_df.iterrows():\n",
    "    filtered_dfs.append(filter_data(row))\n",
    "    \n",
    "\n",
    "# Concatenate all dataframes in filtered_dfs to get a single dataframe\n",
    "final_df = pd.concat(filtered_dfs, ignore_index=True)\n",
    "\n",
    "# Export \n",
    "# final_df.to_csv('input/results_filter.csv', index = False)\n",
    "\n",
    "final_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list\n",
    "cumulativeIDs = []\n",
    "\n",
    "# Loop through N values\n",
    "for N in [1, 2, 3]:\n",
    "    # Loop through M values\n",
    "    for M in range(109, 124):  # This will loop from 127 to 143 inclusive\n",
    "        # Construct the string using the pattern and append to the list\n",
    "        cumulativeIDs.append(f\"{N}_24_{M}\") # 24 = power tab. \n",
    "        \n",
    "# Print the list\n",
    "print(cumulativeIDs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = ['input/WB1.xlsx', 'input/WB2.xlsx', 'input/WB3.xlsx']\n",
    "\n",
    "# Predefined year_to_column mapping.\n",
    "year_to_column = {2020: 4, 2030: 5, 2040: 6, 2050: 7, 2060: 8}\n",
    "\n",
    "# Aggregate the data by UniqueID and Year, summing SATIMGE values.\n",
    "aggregated = final_df.groupby(['UniqueID', 'Year'])['SATIMGE'].sum().reset_index()\n",
    "\n",
    "# Define the range of 'certain years'.\n",
    "start_year = 2020  # adjust accordingly\n",
    "end_year = 2060    # adjust accordingly\n",
    "\n",
    "# Filter out the special rows using the cumulativeIDs list.\n",
    "special_rows = aggregated[aggregated['UniqueID'].isin(cumulativeIDs)]\n",
    "\n",
    "# For these special rows, compute cumulative sums in ten-year steps.\n",
    "cumulative_sums = []\n",
    "\n",
    "for unique_id in cumulativeIDs:\n",
    "    subset = special_rows[special_rows['UniqueID'] == unique_id]\n",
    "    for year in range(start_year, end_year, 10):\n",
    "        end = year + 10\n",
    "        total_sum = subset[(subset['Year'] >= year) & (subset['Year'] < end)]['SATIMGE'].sum()\n",
    "        cumulative_sums.append((unique_id, year, total_sum))\n",
    "\n",
    "# Convert the result into a DataFrame.\n",
    "cumulative_df = pd.DataFrame(cumulative_sums, columns=['UniqueID', 'StartYear', 'CumulativeSATIMGE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_df.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated = pd.merge(aggregated, Units, on='UniqueID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion factors to convert everything to 'm' (for example)\n",
    "conversion_factors = {\n",
    "    'Mt': 0.001, #units in model for emissions are already kt.\n",
    "    'USD': 1/12.94, # to 2015 USD\n",
    "    'kt': 1, #all emissions are in kt\n",
    "    'GW': 1, \n",
    "    'TWh': 1/3.6, #PJ to TWh\n",
    "    'EJ': 0.001, #PJ to EJ\n",
    "    '*': 1,\n",
    "    '': 1,\n",
    "    'million inhab': 0.001,\n",
    "    'gas_Nm3': 25641025,#Nm3 of NGas per PJ. convert PJ of NGas to nM3 of gas    \n",
    "    'crude_Mt':1/42, #Mt of Ngas per PJ. convert PJ of crude oil to Mt of crude oil\n",
    "    'Mtoe': 23884.6, #1 PJ = 23884.6 Mtoe, convert PJ of crude oil/biomass to Mtoe.\n",
    "    'biomass_Mt':1/16, #Mt of biomass per PJ. based on 16MJ/kg convert PJ of biomass to Mt of biomass\n",
    "    'H2_Nm':78740157, #Nm3 per PJ. convert PJ of hydrogen to Nm3\n",
    "    'Mio' : 0.001,\n",
    "    'pkm' : 0.000001, #bpkm default unit in the model\n",
    "    'tkm' : 0.000001, #btkm default unit in the model\n",
    "    'Gvkm' : 1, #bvkm default unit in the model\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "aggregated['SATIMGE'] = aggregated.apply(convert_value, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(conversion_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decades = list(range(start_year, end_year, 10))  # Adjust as per your requirement\n",
    "cumulative_sums = {}\n",
    "\n",
    "for unique_id, group in aggregated.groupby('UniqueID'):\n",
    "    if unique_id in cumulativeIDs:\n",
    "        decade_sums = {}\n",
    "        \n",
    "        for start_year in decades:\n",
    "            end_year = start_year + 10\n",
    "            decade_data = group[(group['Year'] >= start_year) & (group['Year'] < end_year)]\n",
    "            decade_sums[start_year] = decade_data['SATIMGE'].sum()\n",
    "        \n",
    "        cumulative_sums[unique_id] = decade_sums\n",
    "\n",
    "# Print out some of the cumulative_sums for debugging\n",
    "for key, value in list(cumulative_sums.items())[:5]:\n",
    "    print(key, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "aggregated = aggregated[~aggregated['UniqueID'].isin(cumulativeIDs)] #tilda is negation. aggregated NOT in...\n",
    "aggregated = aggregated[aggregated['Year'].isin([2020,2030,2040,2050,2060,2070])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Units['UniqueID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decade_to_column = {\n",
    "    2020: 4,\n",
    "    2030: 5,\n",
    "    2040: 6,\n",
    "    2050: 7,\n",
    "    2060: 8,\n",
    "    2070: 9,\n",
    "    # Adjust as needed\n",
    "}\n",
    "\n",
    "BASE_COLUMN = 4  # Adjust as needed\n",
    "\n",
    "grouped = aggregated.groupby(['UniqueID'])\n",
    "\n",
    "for (unique_id_string,), group in grouped:\n",
    "    workbook_id, sheet_id, row_id = map(int, unique_id_string.split('_'))\n",
    "    print(f\"processing workbook {workbook_id}, sheet {sheet_id}, row {row_id}\")\n",
    "\n",
    "    # Use the workbook_id to select the correct workbook filename\n",
    "    if 0 < workbook_id <= len(file_names):\n",
    "        file_name = file_names[workbook_id - 1]  # Adjust for 0-based indexing\n",
    "    else:\n",
    "        raise ValueError(f\"Workbook ID {workbook_id} is out of range.\")\n",
    "    \n",
    "    try:\n",
    "        book = load_workbook(file_name)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_name} not found. Skipping to the next file.\")\n",
    "        continue\n",
    "\n",
    "    sheet_names = book.sheetnames\n",
    "    if 0 <= sheet_id - 1 < len(sheet_names):\n",
    "        sheet_name = sheet_names[sheet_id - 1]\n",
    "        sheet = book[sheet_name]\n",
    "    else:\n",
    "        raise ValueError(f\"Sheet ID {sheet_id} is out of range for workbook {file_name}.\")\n",
    "\n",
    "    # Check if the unique_id_string is in the special rows\n",
    "    # Check if the unique_id_string is in the special rows and exists in cumulative_sums\n",
    "    if unique_id_string in cumulativeIDs and unique_id_string in cumulative_sums:\n",
    "            print(f'Entering cumulative logic for {unique_id_string}')\n",
    "            for decade_start, sum_val in cumulative_sums[unique_id_string].items():\n",
    "                column_index = decade_to_column[decade_start]\n",
    "                sheet.cell(row=row_id, column=column_index, value=sum_val)\n",
    "    else:\n",
    "        # print(f'Entering non-cumulative logic for {unique_id_string}')\n",
    "        for _, row in group.iterrows():\n",
    "            if row['Year'] in year_to_column:\n",
    "                column_index = year_to_column[row['Year']]\n",
    "                sheet.cell(row=row_id, column=column_index, value=row['SATIMGE'])\n",
    "            else:\n",
    "                print(f\"Warning: Year {row['Year']} not in year_to_column for unique_id_string {unique_id_string}\")\n",
    "\n",
    "    book.save(file_name)\n",
    "\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = merged_df\n",
    "\n",
    "#x[(x['Year'] == 2020)&(x['Subsector'] == 'BioRef')&(x['Indicator']=='FlowOut')]['Short Description'].unique()\n",
    "\n",
    "x[(x['Commodity']=='IFACHA')]\n",
    "#x['Commodity'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
